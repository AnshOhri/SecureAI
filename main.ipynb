{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_file_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRIEFING  \\nEU Legislation in Progress  \\n \\nEPRS | European Parliamentary Research  Service  \\nAuthor: Tambiama  Madiega  \\nMembers\\' Research Service \\nPE 698.792  –  March 2024  EN \\nArtificial intelligence act  \\nOVERVIEW  \\nEuropean Union lawmakers reached a political agreement on the draft artificial intelligence (AI) act \\nin December 2023. Proposed by the European Commission in April  2021, t he draft AI act, the first \\nbinding  worldwide  horizontal regulation  on AI, sets a common framework for the use and supply of \\nAI systems in the EU. It  offers  a classification for AI sy stems with different requirements and \\nobligations tailored on a \\'risk-based approach \\'. Some AI systems presenting \\'unacceptable \\' risks are \\nprohibited. A wide range of \\'high -risk\\' AI systems that can have a detrimental impact on people\\' s \\nhealth, safety or o n their fundamental rights  are authorised, but subject to a set of requirements and \\nobligations to gain access to the EU market. AI systems posing limited risks because of their lack of \\ntransparency will be subject to information and transparency requirements,  while AI systems \\npresenting only minimal risk  for people will not be subject to further obligations. The regulation also \\nprovides specific rules for general purpose AI (GPAI) models  and lays down more stringent \\nrequirements for GPAI models with \\'high -impact capabilities \\' that could pose a systemic risk and \\nhave a significant  impact on the internal market.  \\nThe provisional agreement has been endorsed by the Committee of Permanent Representatives of \\nEU Member States and by Parliament\\'s two lead committee s. Parliament\\'s plenary vote on the final \\nagreement is scheduled for the March plenary session. The AI act must also be endorsed by Council \\nand published in the EU \\'s Official Journal  before entering into force.  \\nProposal for a regulation of the European Par liament and of the Council laying down harmonised \\nrules on artificial intelligence (artificial intelligence act) and amending certain Union legislative \\nacts  \\nCommittees \\nresponsible:  \\n \\nRapporteurs:  \\n \\nShadow rapporteurs:  Internal Market and Consumer Protection  (IMCO) and \\nCivil Liberties, Justice and Home Affairs (LIBE) (jointly \\nunder Rule 58)  \\nBrando Benifei (S&D, Italy)  and Dragoş Tudorache \\n(Renew, Romania) \\nDeirdre Clune, Axel Voss (EPP); Petar Vitanov (S&D); \\nSvenja Hahn, (Renew); Sergey Lagodinsky, Kim  Van  Sparrentak (Greens/EFA); Rob Rooken, \\nKosma Złotowski (ECR); Jean -Lin Lacapelle, Jaak \\nMadison (ID); Cornelia Ernst, Kateřina Konecna (The \\nLeft)  COM(2021)206  \\n21.4.2021  \\n2021/0106(COD) \\n \\nOrdinary legislative \\nprocedure (COD) \\n(Parliament and \\nCouncil on equal footing  – formerly \\n\\'co-decision \\') \\nNext steps expected:  Final first -reading vote in plenary  \\n \\nEPRS | European Parliamentary Research Service  \\n2 Introduction  \\nAI technologies are expected to bring a wide array of economic and societal benefits  to a wide \\nrange of sectors, including environment and health, the public sector, finance, mobility, home affairs \\nand agriculture. They are particularly useful for improving prediction, for optimising operations and \\nresource allocation, and for personalisi ng services.1 However, the implications of AI systems for \\nfundamental rights  protected under the EU Charter of Fundamental Rights, as well as the safety \\nrisks  for users when AI technologies are embedded in products and services, are raising concern. \\nMost notably, AI systems may jeopardise fundamental rights such as the right to non -discrimination, \\nfreedom of expression, hu man dignity, personal data protection and privacy.2 \\nGiven the fast development of these technologies, in recent years AI regulation has become a \\ncentral policy question in the European Union (EU). Policy -makers pledged  to develop a \\' human -\\ncentric\\'  approach to AI  to ensure that Europeans can benefit from new technologies developed \\nand functioning according to the EU \\'s values and principles. In its 2020 White Paper on Artificial \\nIntelligence, the European Commission committed to promote the uptake of  AI and address the \\nrisks associated  with certain uses of this new technology. After having  initially adopted a soft -law \\napproach  with the publication of its non -binding 2019 Ethics Guidelines for Trustworthy AI  and \\nPolicy and investment recommendations , the European Commission shifted  towards a legislative \\napproach , calling for the adoption of harmonised rules  for the development, placing on the market \\nand use of AI systems.  \\nLeading the EU -level debate, the Parliament called on the Commission to assess the impact of AI \\nand to draft an EU framework for AI, in its wide -ranging 2017 recommendations on civil law rules on \\nrobotics . In 2020 and 2021, Parliament adopted a number of non -legislative resolutions calling for \\nEU action ,3 as well as two legislative resolutions asking the Commission to establish a legal \\nframework of ethical principles  for the development, deployment and use of AI, robotics and related \\ntechnologies in the Union and harmonising  the legal framework for civil liability  claims and \\nimposition of a regime of strict liability on operators of high -risk AI systems.  \\nIn the past, the Council has repeatedly called for the adoption of common AI rules, incl uding in 2017  \\nand 2019. In 2020, the Council called  upon the Commission to put forward concrete proposals that \\ntake existing legislation into account and follow a risk -based, proportionate and, if necessary, \\nregulatory approach.  \\nThe Commission launched a broad public consultation  in 2020 and published an  Impact Assessment \\nof the regulation on artificial intelligence, a  supporting study  and a draft proposal , wh ich received \\nfeedback  from stakeholders.4 In its impact assessment, the Commission identifie d several problems  \\nraised by the development and use of AI systems, due to their specific characteristics , namely:  \\n(i) opacity (limited ability of the human mind to understand how certain AI systems operate), \\n(ii) complexity, (iii)  continuous adaptation and unpredictability, (iv)  autonomous behaviour, and \\n(v)  functional dependence on data and on the quality of data.  \\nAI regulatory approach in the world . An increasing number of countries  worldwide are designing and \\nimplementing AI governance legislation and policies . While the United States of America (USA) had initially \\ntaken a lenient approach towards AI, calls  for regulation have recen tly been mounting. The White House has \\nreleased the Blueprint for an AI Bill of Rights , a set of guidelines to protect the rights of the American public in \\nthe age of AI and President Joe Biden signed an executive order on AI  in 2023.  The Cyberspace A dministration \\nof China issued some guidelines  on generative AI services, while the UK has announced  a pro -innovation \\napproach to AI regulation, which largely regulates AI via existing laws. At international level, the Organisation \\nfor Economic Co -operation and Development (OECD) adopted some non -binding Principles on AI , in 2019, \\nUNESCO embraced a set of Recommendations on the Ethics of AI  in 2021, the G7 agreed some International \\nGuiding Principles on Art ificial Intelligence in 2023 and the Council of Europe is currently finalising an \\ninternational convention on AI . Furthermore, in the context of the newly established EU -US tech partnership \\n(the Trade and Technology Council), the EU and the USA are seeking to develop a mutual understanding on \\nthe principles underpinning trustworthy and responsible A I. Artificial intelligence act  \\n3 The changes the proposal would bring  \\nThe draft AI act was  designed as a horizontal EU legislative instrument  applicable to all AI systems \\nplaced on the market or used in the Union,  based on Article  114 and Article  16 of the Treaty on the \\nFunctioning of the European Union (TFEU)  following the logic of the new legislative framework  \\n(NLF), i.e. the EU \\'s approach to ensuring a range of products comply with the applicable legislation \\nwhen they are placed on the EU market through conformity assessments and the use of CE marking.   \\nThe Commission proposed enshrining  in EU law a legal  definition  of \\'AI system \\' referring  to a range \\nof software -based technologies  using specific techniques and approaches (\\' machine learning \\', \\n\\'logic and knowledge -based \\' systems, and \\' statistical \\' approaches ) that could be complemented  \\nthrough the adoption of delegated acts  to facto r in technological developments .  \\nThe Commission also proposed to adopt a risk -based approach  whereby legal intervention was  \\ntailored to concrete level of risk. Four categories were identified.  \\nFirst, the draft act proposed to explicitly ban the following  harmful AI practices  that are considered \\nto be a clear threat to people \\'s safety, livelihoods and rights, because of the \\' unacceptable risk \\' they \\ncreate:  \\n\\uf0d8 AI systems that deploy harmful manipulative \\'subliminal techniques\\';  \\n\\uf0d8 AI systems that exploit specific vulnerable groups (physical or mental disability);  \\n\\uf0d8 AI systems used by public authorities, or on their behalf, for social scoring purposes;  \\n\\uf0d8 \\'Real -time \\' remote biometric identification systems in publicly accessible spaces for \\nlaw enforcement purpos es, except in a limited number of cases.5 \\nSecond, the draft act proposed to regulate  high -risk  AI systems  that create adverse impact on \\npeople \\'s safety or their fundamental rights. The draft text distinguishe d between two categories of \\nhigh -risk AI systems.  \\n\\uf0d8 Systems used as a safety component of a product or falling under EU health and safe ty \\nharmonisation legislation  (e.g. toys, aviation, cars, medical devices, lifts).  \\n\\uf0d8 Systems deployed in eight specific areas  specified in Annex (e.g. law enforcement),  \\nwhic h the Commission could update as necessary through delegated acts.   \\nSuch high- risk AI systems would have to comply with a range of requirements  particularly on risk \\nmanagement, testing, technical robustness, data training and data governance, transparency,  \\nhuman oversight, and cybersecurity  before being placed on the market or put  into service. AI \\nsystems that conform to new harmonised EU standards  would benefit from a presumption of \\nconformity with the draft AI act requirements. \\nThird, AI systems presentin g limited risk , such as systems that interacts with humans (i.e. chatbots), \\nemotion recognition systems, biometric categorisation systems, and AI systems that generate or \\nmanipulate image, audio or video content (i.e. deepfakes ), would be subject to a limi ted set of \\ntransparency obligations.  \\nFinally, all other AI systems presenting only low or minimal risk  could be developed and used in \\nthe EU without conforming to any additional legal obligations. However, the proposed AI act \\nenvisage d the creation of codes of conduct  to encourage providers of non- high -risk AI systems to \\napply the mandatory requirements for high- risk AI systems voluntarily . \\nThe proposal required  Member States to designate one or more competent authorities, including a \\nnational superviso ry authority , which would be tasked with supervising the application and \\nimplementation of the regulation, and  proposed to establish  a European Artificial Intelligence \\nBoard  (composed of representatives from the Member States and the Commission) at EU level. \\nNational market surveillance authorities  would be responsible for assessing operators \\' \\ncompliance with the obligations and requirements for high- risk AI systems. Administrative fines  of EPRS | European Parliamentary Research Service  \\n4 varying scales (up to €30  million or 6  % of the total worldwide ann ual turnover), depending on the \\nseverity of the infringement, were  set as sanctions for non -compliance with the AI a ct.  \\nSome measures were tailored to foster investments. The Commission propose d that Member States, \\nor the European Data Protection Supervis or, could establish a regulatory sandbox , i.e. a controlled \\nenvironment that facilitates the development, testing and validation of innovative AI systems (for a \\nlimited period of time) before they are put on the market. Sandboxing w ould  enable participants  to \\nuse personal data to foster AI innovation, without prejudice to the GDPR  requirements. Other \\nproposed measures were  tailored specifically to small -scale providers and start- ups .   \\nAdvisory c ommittees  \\nThe European Economic and Social Committee and the European Committee of the Regions \\nadopted  their opinions in 2021 and in 2022,  respectively.  \\nNational parliaments  \\nThe deadline for the submission of reasoned opinions  on the grounds of subsidiarity was \\n2 September  2021. Contributions were received from the Czech Chamber of Deputies  and the Czech \\nSenate , the Portuguese Parliament , the Polish Senate  and the German Bundesrat .  \\nStakeholder views6 \\nDefinitions  were  a contentious point of discussion among stakeholders. The Big Data Value \\nAssociation, an industry -driven international not –for -profit organisation, stresse d that the definition \\nof AI systems was  quite broad and would cover far more than what is subjectively understood as AI, \\nincluding the simplest search, sorting and routing algorithms, which would consequently be subject \\nto new rules. Further more, they ask ed for clarification of how components of larger AI systems (such \\nas pre -trained AI components from other manufacturers or components not released separately), \\nshould be treated. AmCham, the American Chamber of Commerce in the EU, suggest ed avoiding \\nover -regulation by adopting a narrower definition of AI systems, focusing strictly on high -risk AI \\napplications (and not extended to AI applications that are not high -risk, or software in general).  \\nWhile they generally welcome d the proposed AI act \\'s risk -based approach , some stakeholders \\nsupport ed wider prohibition and regulation of AI systems. Civil rights organisations call ed for a ban \\non indiscriminate or arbitrarily targeted use of biometrics in public or publicly accessible spaces, and for restrictions on the uses of AI systems, including for border control and predictive policing.  \\nThe European Enterprises Alliance stresse d that there was  general uncertainty about the roles and \\nresponsibilities of the different actors in the AI value chain (developers, providers, and users of AI systems). This was  particularly challenging for companies providing general purpose application \\nprogramming interfaces or open -source AI models  that are not specifically intended for high- risk \\nAI systems but are nevertheless used by third parties in a manner that could be considered high-\\nrisk. They also call ed for \\'high -risk\\' to be redefined, based on the measurable harm and potential \\nimpact. AlgorithmWatch underlined  that the applicability of specific rules should not depend on the \\ntype of technology, but on the impact it has on individuals and society. They call ed for the new rules \\nto be defined according to the impact of the AI systems and recommend that every operator should \\nconduct an impact assessment that assesses the system \\'s risk levels on a case -by-case basis. Cli mate \\nChange AI call ed for climate change mitigation and adaptation to be taken into account i n the \\nclassification rules for high -risk AI systems and impose environmental protection requirements.  \\nThe European Consumer Organisation, BEUC, stre ssed that the proposal required  substantial \\nimprovement to guarantee consumer protection . The organisation argue d that the proposal \\nshould have a broader scope and impose basic principles and obligations (e.g. on fairness, \\naccountability and transparency) upon all AI systems, as well as prohibiting more comprehensively \\nharmful practices (such as private entities \\' use of social scoring and of remote biometric Artificial intelligence act  \\n5 identification systems in public spaces). Furthermore, consumers should be granted a strong set of \\nrights, effective remedies and redress mechanisms, including collective redress.  \\nThere were  opposing views on the impact of the proposed regulation on investment . A study  by \\nthe Centre for Data Innovation (representing large online platforms) highlighted  that the \\ncompliance costs incurred under the proposed AI act would likely provoke a chilling effect on \\ninvestment in AI in Europe, and could particularly deter small and medium -sized enterpr ises (SMEs) \\nfrom developing high- risk AI systems. According to the study , the AI act would cost the European \\neconomy €31  billion over the next five years and reduce AI investments by almost 20  %. However, \\nsuch estimates of the compliance costs were  challenged by the experts  from the Centre for \\nEuropean Policy Studies, as well as by other ec onomists. The European Digital SME Alliance warned  \\nagainst overly stringent conformity requirements, and asked  for effective SME representation in the \\nstandards- setting procedures and for mandatory sandboxes in all EU Member States.  \\nAcademic and other views  \\nWhile generally supporting the Commission \\'s proposal, critics call ed for amendments, including \\nrevising the \\'AI systems\\'  defin ition, ensuring a better allocation of responsibility, strengthening \\nenforcement mechanisms and fostering democratic participation.7 Among the main issues were:  \\nAI systems definition  \\nThe legal definition of \\'AI systems\\'  contained in the proposed AI act ha s been heavily criticised . \\nSmuha and others warned  the definition lacks clarity and may lead to legal uncertainty, especially \\nfor some systems that would not qualify as AI systems under the draft text, while their use may have \\nan adverse impact on fundamental rights.8 To address this issue, the authors propose d to broaden \\nthe scope of the legislation  to include explicitly all computational systems used in the identified \\nhigh -risk domains, regardless of whether they are considered to be AI. Ebers and others consider  \\nthat the scope of \\' AI systems\\'  was overly broad, which may lead to legal uncertainty  for developers, \\noperators, and users of AI systems and ultimately to over -regulation.9 They c alled on EU law -makers \\nto exempt AI systems developed and used for research purposes  and open -source software  (OSS) \\nfrom regulation. Other commentators question ed whether the proposed definition of \\' AI systems\\'  \\nis truly technology neutral  as it refers primarily to \\' software \\', omitting potential future AI \\ndevelopments. \\nRisk -based approach  \\nAcademics also call ed for amendments, warning that the risk -based approach proposed by the \\nCommission would not ensure a high level of protection of fundamental rights. Smuha and others \\nargue d that the proposal does not always accurately recognise  the wrongs and harms associated \\nwith different kinds of AI systems and therefore does not appropriately allocate responsibility. \\nAmong other things, they recommend ed adding a proc edure that enables the Commission to \\nbroaden the list of prohibited AI systems , and propose d banning existing manipulative AI \\nsystems (e.g. deepfakes), social scoring and some biometrics. Ebers and others call ed for a more \\ndetailed classification of risks  to facilitate industry self -assessment and support, as well as \\nprohibiting more AI systems  (e.g. biometrics), including in the context of private use . \\nFurthermore, some highlight ed that the draft legislation did not address systemic sustainability \\nrisks  created by AI , especially in the area of climate and environmental protection.10  \\nOne of the major concerns raised was  that the rules on prohibited and high- risk practices might  \\nprove ineffective in practice, because the risk assessment was proposed to be  left to provider self -\\nassessment . Veale and Zuiderveen Borgesius warn ed that most providers can arbitrarily classify \\nmost high -risk systems as adhering to the rules using self -assessment procedures alone. Smuha and \\nothers recommend ed exploring whether certain high- risk systems would not benefit from a \\nconform ity assessment carried out by an independent entity  prior to their deployment.  EPRS | European Parliamentary Research Service  \\n6 Biometrics regulation.  A study commissioned by the European Parliament recommended , inter alia, to \\nempower the Commission to adapt the list of prohibited AI practices periodically, under the supervision of the \\nEuropean Parliament, and the adoption of a more comprehensive list of \\'restricted AI applications\\' (comprising \\nreal-time remote biometric identification without limitation for law enforcement purposes). Regulation of \\nfacial recognition technologies (FRTs) was one of the most contentious issues.11 The European Data Protection \\nSupervisor (EDPS) and the European Data Protec tion Board (EDPB) called  for a general ban on any uses of AI \\nfor the automated recognition of human features in publicly accessible spaces.  \\nGovernance structure and enforcement and redress mechanisms  \\nEbers et al.  stressed  that the AI act lacks effective enforcement structures , as the Commission \\npropose d to leave the  preliminary risk assessment, including the qualification as high- risk, to the \\nproviders \\' self-assessment. They also raise d concerns about the excessive delegation of regulatory \\npower to private European standardisation organisations (ESOs), due to the lac k of democratic \\noversight, the impossibility for stakeholders (civil society organisations, consumer associations) to \\ninfluence the development of standards, and the lack of judicial means to control them once they have been adopted. Instead, they recommen ded that the AI act codify  a set of legally binding \\nrequirements for high- risk AI systems, which ESOs may specify through harmonised standards.  \\nCommentators regretted  a crucial gap in the AI act  – the lack of provision  provide for individual \\nenforcement r ights . Ebers and others stressed  that individuals affected by AI systems and civil \\nrights organisations have no right to complain  to market surveillance authorities or to sue a \\nprovider or user for failure to comply with the requirements. Similarly, Veale and Zuiderveen \\nBorgesius warn ed that, while some provisions of the draft legislation aim to impose obligations on \\nAI systems users, no mechanism for complaint or judicial redress  was available to them. Smuha \\nand others recommend ed amending the proposal to include, inter alia, an explicit right of redress \\nfor individuals  and rights of consultation and participation for EU citizens  regarding the \\ndecision to amend the list of high- risk systems in Annex  III. \\nIt has also been stressed  that the text proposed by the Commission  lack ed proper coordination  \\nmechanisms between authorities, in particular concerning cross -border infringement . \\nFurthermore, guidance would be desirable  on how to ensure compliance with trans parency and \\ninformation requirements, while simultaneously protecting intellectual property rights and \\ntrade secrets , not least to avoid diverging practices in the Member States.  \\nLegislative process  \\nThe Council  adopted its common position  in December 2022. In Parliament , the file was assigned \\njointly (under Rule  58) to the Commit tee on Internal Market and Consumer Protection (IMCO) and \\nthe Committee on Civil Liberties, Justice and Home Affairs (LIBE), with Brando  Benifei (S&D, Italy) and \\nDrago ş Tudorache, Renew, Romania) appointed as rapporteurs. Parliament adopted  its negotiating \\nposition (499  votes in favour, 28  against and 93  abstentions) in June  2023, with substantial \\namendments  to the Commission \\'s text.  Following protracted negotiations, the Council and the \\nEuropean Parliament  reached a provisional agreement  on the AI act on 9  December 2023. The \\nEuropean Parliament \\'s LIBE and IMCO committees endorsed  the final text  in a joint vote on \\n13 February  2024, with an overwhelming majority (71  votes in favour, 8  votes against and \\n7 abstentions).  The European Parliament will now vote on the final agreement on the AI act at the \\nMarch  2024 plenary  session, before it is endorsed by Council ands published in the EU\\'s Official \\nJournal. The main points of the EU AI rules are:  \\nDefinitions  \\nThe AI act enshrines in EU law a definition  of AI systems  aligned with the revised definition agreed \\nby the OECD :  Artificial intelligence act  \\n7 \\'An AI system is a machine -based system designed to operate with varying levels of autonomy and \\nthat may exhibit adaptiveness after deployment and that, for explicit or implicit objectives, infers, \\nfrom the input it receives, how to generate outputs such as predictions, content, recommendations, \\nor decisions that can influence physical or virtual environments \\'.  \\nThe definition is not intended to cover simpler traditional software sys tems or  programming \\napproaches, and the Commission has been tasked to develop guidelines  on its application.    \\nThe act also contains a definition of g eneral purpose  artificial intelligence  (GPAI) models  \\'that \\nare trained with a large amount of data using self -supervision at scale \\', that display \\'significant \\ngenerality \\' and are \\'capable to competently perform a wide range of distinct tasks\\'  and \\'can be \\nintegrated into a variety of downstream systems or applications\\' . Furthermore, the AI act defines \\ngeneral -purpose AI systems  as systems based on a GPAI model, which have the capability to serve \\na variety of purposes, both for direct use as well as for integration in other AI systems.   \\nScope of application   \\nThe AI act  applies  primarily to  providers and deployers putting AI systems and GPAI models into \\nservice or placing  on the EU market and who have their place of establishment or who are located \\nin the EU, as well as to deployers or providers of AI systems that are es tablished in a third country, \\nwhen the output produced by their systems is used in the EU .12 However, AI systems placed on the \\nmarket, put into service, or used by public and private entit ies for military, defence or national \\nsecurity  purposes, are excluded from the scope. Similarly, the AI a ct will not apply to AI systems and \\nmodels, including their output , which  are specifically developed and put into service for the sole \\npurpose of scientific research and development . Furthermore, as matter of  principle, the \\nregulation does not apply prior to the  systems and models being put into service or placed on the \\nmarket  (sandboxing rules may apply in this case).  \\nRisk -based approach \\nEU AI act risk -based approach  \\n \\nData source: European Commission  \\nEPRS | European Parliamentary Research Service  \\n8 The final agreement maintains the risk -based approach proposed by the Commission and classifies \\nAI systems into several risk categories, with different degrees of regulatio n applying .  \\n\\uf0d8 Prohibited AI practices . The final text prohibits a wider range of AI practices as \\noriginally proposed by the Commission because of their harmful impact:   \\n\\uf0d8 AI systems using subliminal or manipulative or deceptive techniques  to \\ndistort people \\'s or a group of people\\'s behaviour and impair informed \\ndecision -making, leading to significant harm;  \\n\\uf0d8 AI systems exploiting vulnerabilities due to age, disability, or social or \\neconomic situations, causing significant harm;  \\n\\uf0d8 Biometric categorisation systems inferring race, political opinions, trade \\nunion membership, religious or philosophical beliefs, sex life, or sexual orientation (except for lawful labelling or filtering in law -enforcement \\npurpose s);  \\n\\uf0d8 AI systems evaluating or classifying individuals or gr oups based on social \\nbehaviour or personal characteristics, leading to detrimental or disproportionate treatment in unrelated contexts or unjustified or disproportionate to their behaviour;  \\n\\uf0d8 \\'Real -time \\' remote biometric identification in public spaces for law \\nenforcement (except for specific necessary objectives such as searching for \\nvictims of abduction, sexual exploitation or missing persons, preventing \\ncertain substantial and imminent threats to safety, or identifying suspects in serious crimes);  \\n\\uf0d8 AI systems assessing the risk of individuals committing criminal offences \\nbased solely on profiling or personality traits and characteristics (except when \\nsupporting human assessments based on objective, verifiable facts linked to \\na criminal activity);  \\n\\uf0d8 AI system s creating  or expanding  facial recognition databases through \\nuntargeted scraping from the internet or CCTV footage;  \\n\\uf0d8 AI systems inferring emotions in workplaces or educational institutions, \\nexcept for medical or safety reasons.  \\n\\uf0d8 High -risk AI systems.  The A I act identifies  a number of use cases in which AI systems  \\nare to be considered high  risk because they can  potenti ally create an adverse impact \\non people\\'s health, safety or their fundamenta l rights.   \\n\\uf0d8 The risk classification  is based on the intended purpos e of the AI system. The \\nfunction performed by the AI system and the specific purpose and modalities \\nfor which the system is used are key to determine if an AI system is high -risk \\nor not. High -risk AI systems can be safety components of products covered by \\nsectoral EU law  (e.g. medical devices) or AI systems that, as a matter of \\nprinciple , are considered to be high- risk when they are used in specific areas  \\nlisted in an annex.13 The Commission is tasked with maintaining an EU \\ndatabase for  the high -risk AI  systems listed in this annex.  \\n\\uf0d8 A new test has been enshrined at the Parliament\\'s request (\\' filter provision \\'), \\naccording to which  AI systems will not be considered high -risk if they do not \\npose a significant risk of harm to the health, safety or fundament al rights of \\nnatural persons.14 However, an AI system will always be considered high- risk \\nif the AI system performs profiling of natural persons.  \\n\\uf0d8 Providers of such high -risk AI systems will have to run a conformity \\nassessment  procedure  before their products  can be sold and used in the EU. \\nThey will need to comply with a range of requirements including for testing , \\ndata training  and  cybersecurity and , in some cases, will have to conduct a \\nfundamental rights impact assessment to e nsure their systems compl y with \\nEU law. The conformity assessment should be carried out either based on Artificial intelligence act  \\n9 internal control (self -assessment) or with the involvement of a notified body \\n(e.g. biometrics). C ompliance with European harmonised standard s to be \\ndeveloped  will grant high- risk AI  systems providers a presumption of \\nconformity.  After such AI systems are placed in the market, providers must \\nimplement p ost-market monitoring and take corrective actions if necessary. \\n\\uf0d8 Transparency risk . Certain AI systems intended to interact with natural persons or to \\ngenerate content may pose specific risks of impersonation or deception,  irrespective \\nof whether they qualify as high- risk AI systems or not. Such systems are subject to \\ninformation and tra nsparency requirements. Users must be made aware that they \\ninteract with chatbots. D eployers of AI systems that generate or manipulate image, \\naudio or video content (i.e. deep fakes ), must disclose that the content has been \\nartificially generated or manipu lated except in very limited cases (e.g. when it is used \\nto prevent criminal offences ). Providers of AI systems that generate large quantities of \\nsynthetic content  must implement sufficiently reliable, interoperable, effective and \\nrobust techniques and met hods (such as watermarks) to enable marking and \\ndetection that the output has been generated or manipulated by an AI system and \\nnot a human. Employers who deploy AI systems in the workplace  must inform the \\nworkers and their representatives.  \\n\\uf0d8 Minimal risks . Systems presenting minimal risk for people (e.g. spam filters) will not \\nbe subject to further obligations beyond  current ly applicable legislation (e.g., GDPR).   \\n\\uf0d8 General -purpose AI (GPAI) . The r egulation provides specific rules for general -\\npurpose AI models  and for general -purpose AI models that pose systemic risks.  \\n\\uf0d8 GPAI system transparency requirements. All GPAI models will have to draw \\nup and maintain  up-to-date technical documentation and make information \\nand documentation available to downstream providers of AI systems. A ll \\nproviders of GPAI models have to put a policy in place to respect Union \\ncopyright law , including through state -of-the-art technologies (e.g. \\nwatermarking), to carry out lawful text- and -data mining exceptio ns as \\nenvisaged  under the Copyright D irective. Furthermore, GPAIs must draw up \\nand make publicly available a sufficiently detailed summary of  the content \\nused in training the GPAI models according to a te mplate provided by the \\nAI Office .15 Finally, if located outside the EU, they will have to appoint a \\nrepresentative in the EU . However, AI models made accessible under a free \\nand open source  will be exempt from some of the obligations (i.e. disclosure \\nof tec hnical documentation) given they have, in principle, positive effects on \\nresea rch, innovation and competition .16  \\n\\uf0d8 Systemic -risk GPAI obligations . GPAI models with \\' high -impact \\ncapabilities \\' could pose a systemic risk and have a significant impact on the \\ninternal market, due to their reach and their actual or reasonably foreseeable negative effects (on public health, safety, public security, fundamental rights, or the society as a whole). GPA I providers must therefore notify the European \\nCommission if their model is trained using a total computing power  \\nexceeding 10 ^25 FLOPs (i.e. floating- point operations per second). When this \\nthreshold is met, the presumption will be that the model is a GPA I model \\nposing  systemic risks.\\n17 In addition to the requirements on transparency and \\ncopyright protection falling on all GPAI models, providers of systemic -risk \\nGPAI models are required to constantly assess and mitigate  the risks  they \\npose and to ensu re cybersecurity protection. That requires, inter alia, keep ing \\ntrack of, document ing and report ing serious incidents (e.g. violations of \\nfundamental rights) and implement ing corrective measures .  \\n\\uf0d8 Code s of practice  and presumption of conformity. GPAI model  provi ders  \\nwill be able to rely on c odes of p ractice to demonstrate compliance with the EPRS | European Parliamentary Research Service  \\n10 obligations  set under the act. By means of implementing acts, the \\nCommission may decide to approve a code of practice and give it a general \\nvalidity within the EU, or alternatively, provide common rules for \\nimplement ing the relevant obligations . Compliance with a European \\nharmonised standard grants GPAI providers the presumption of conformity . \\nProviders of GPAI models with systemic risks who do not adhere to an \\napproved code of practice will be required to demonstrate adequate \\nalternative means of complianc e.  \\nSandboxing and real- world testing  \\nThe measures to support investment  in AI systems have been strengthened. National  authorities \\nmust establish at least one AI regulatory sandbox at national level to facilitate the development and \\ntesting of innovative AI systems under strict regulatory oversight.18 Such regulatory sandbox es \\nprovide for a controlled environment that fosters innovation and facilitates the developm ent, \\ntraining, testing and validation of innovative AI systems for a limited time before their placement on the market or entry  into service . The AI regulatory sandbox must enable, where appropriate, t esting \\nof AI systems in real -world conditions outside o f a laboratory for a limited period (subject to \\ncompliance with EU data protection law rules and principles). Furthermore, to accelerate the \\ndevelopment and placing on the  market of high -risk AI systems, providers or prospective providers \\nof such  systems may also test them in real -world conditions  – even without participating  in an AI \\nregulatory sandbox  – if they respect some guarantees and conditions  (e.g. ask for specific consent, \\nsubmit their real -world testing plan to the market surveillance a uthori ty).  \\nEnforcement and institutional setting  \\nThe implementation of the act will be the responsibility of a number of national and EU -level actors. \\nMember States must establish or designate at least one market surveillance authority and at least \\none notifying authority  to ensure the application and implementation of the act. Heavy fines  will \\nfall on non -compliant entities .19 At EU level , a range  of actors including the Commission, the AI \\nBoard, the AI O ffice, the EU standardisation bodies (CEN and CENELE C) and an advisory forum and \\nscientific panel of independent experts  will support the implementation of the act. The EU AI Office  \\nwas  established to provide advice on the implementation of the new rules , in particular as regards \\nGPAI  models and to develop codes of practice to support the proper application of the AI a ct.   \\n\\'Entry into force \\' timelines  \\nProhibited systems have to be phased out within six months  after the act enters into force. The \\nprovisions concerning GPAI and penalties will apply 12 months  after the act enters into force , and \\nthose concerning high -risk AI systems apply 24 months  after entry into force (36 months after  entry \\ninto force  for AI systems covered by existing EU product legislation) . The c odes of practice envisaged \\nmust be ready , at the latest,  nine months after the AI act enters into force.  The implementation of \\nthe AI a ct requires a number of steps to be taken. In the coming months, t he Commission is expected \\nto issue various implementing, delegated and guidelines  related to the a ct20 and to oversee the \\nstandardisation process  required for implementing the obligations .21 \\nPolicy debate latest issues . Academics have raised a number of questions as regard s the final text of the AI \\nact and the implementation challenges lying ahea d. Hacker welcomes the final AI act text but stresses, inter \\nalia: that alignment with existing sectoral regulation is incomplete (which results in unnecessary and highly \\ndetrimental red tape);  compliance costs will be substantial,  especially for SMEs developing narrow AI models ; \\nthe threshold of 10^25 FLOPs for a default categori sation of systemic risk models is too high ; and calls for \\nEuropean supervision and monitoring of remote biometric identification to avoid the risk that some Member \\nStates  circumven t the rules enshrined in the AI act.22 Kutterer argues the AI act \\'s implementation will require \\na robust taxonomy setting out the correlation of risk classification and model capabilities and asses sing  the \\ndevelopments of open sources models.23 Helberger and others call for  the AI act to be complemented by  an \\nadditional set of exercisable rights to protect citizens from  AI-generated harm, with additional legislation to Artificial intelligence act  \\n11 control the potential environmental impact of training AI models and protect worker\\'s rights and to define \\nfurther a set of requirements that research organisations must comply with to benefit from the research \\nexemption.24 Also, some argue  that the AI act d oes not go far enough in preventing and/or mitigating the \\nspecific risks associated with chatbots. T imely standardisation will be key to ensur ing adequate \\nimplementation of the AI act, for instance, to ensure the robustness of high -risk AI systems and the \\nwatermarking  of AI -generated content while, in the meantime, the EU is fostering the adoption of voluntary \\ncodes of conduct  and of an AI Pact  to mitigate the potential downsides of generative AI. Some academics \\nwarn  that that the standardisation and codification processes might not include representative groups of \\nstakeholders and risks privileging regulated parties. Ensuring  international harmonisation  of AI governance \\nhas become a key topic for policymakers. More cooperation on aligning  AI governance between the EU and \\nthe US A is seen as crucial for AI\\'s democratic governance.25 Key questions such as setting a common \\nterminology  and addre ssing dual -use and military AI applications  have been raised in this respect. Finally, \\ngenerative AI is seen as a disruptive technology that will likely mean amending EU laws and regulation , \\nincluding in intellectual property rights , privacy and data protection and cyb ersecurity.  \\nEUROPEAN PARLIAMENT SUPPORTING ANALYSIS  \\nAI Reposito ry, EPRS, STOA Centre for Artificial Intelligence (C4AI), October 2023.  \\nBiometric Recognition and Behavioural Detection , Policy Department for Citizens \\' Rights and \\nConstitutional Affairs, August 2021.  \\nDalli H., Artificial Intelligence Act: Initial Appraisal of the European Commission Impact Assessment , EPRS, \\nJuly 2021.  \\nDumbrava C., Artificial intelligence at EU borders: Overview of applications and key issues , EPRS, \\nJuly 2021.  \\nMadiega T. A. and Mildebrath H. A., Regulating  facial recognition in the EU , EPRS, September 2021.  \\nMadiega T., Artificial intelligence act and regulatory sandboxes , EPRS, March 2022.  \\nMadiega T., General -purpose artificial intelligence, EPRS, March 2023.  \\nMadiega T., Generative AI and watermarking , EPRS, December  2023.  \\nOTHER SOURCES  \\nArtificial Intelligence Act, European Parliament, Legislative Observatory (OEIL).  \\nNovelli C. et al. , Generative AI in EU Law: Liability, Privacy, Intellectual Property, and Cybersecurity , 2024.  \\nHacker P., Comments on the f inal trilogue version of the AI a ct, 2024.  \\nENDNOTES\\n1  See European Commission, Proposal for a regulation of the European Parliament and of the Council laying down \\nharmonised rules on artificial intelligence ( artificial intelligence act) 2021/0106 (COD) , Explanatory memorandum.  \\n2  See for instance, High -Level Expert Group, Ethics Guidelines for Trustworthy AI , 2019.  \\n3  See, inter alia, Recommendations on  intellectual property , criminal law , education, culture and audiovisual  areas, \\nand regarding civil and military AI uses . \\n4  For an overview s ee H. Dalli, Artificial intelligence act , above .  \\n5  It was proposed to allow FRTs  (i) for targeted search for potential  victims of crime, including missing children ; (ii) to \\nprevent a specific, substantial and imminent threat to the life or physical safety of persons or of a terrorist attack ; \\nand (iii)  for the detection, localisation, identification or prosecution of a per petrator or individual suspected of a \\ncriminal offence referred to in the European Arrest Warrant Framework Decision . \\n6  This section aims to provide a flavour of the de bate and is not intended to be an exhaustive account of all different \\nviews on the proposal. Additional information can be found in publications listed under \\'supporting analysis\\'.  \\n7  For an in -depth analysis of the proposals and recommendations for amendm ents see N. Smuha et al. , How the EU \\ncan achieve legally trustworthy AI: A response to the European Commission \\'s proposal for an a rtificial intelligence \\nact, Elsevier, August 2021; M. Ebers, and others, The European Commission’s proposal for an a rtificial intelligence \\nact—A critical assessment by members of the Robotics and AI Law Society (RAILS) , J 4, no  4: 589-603, October 2021.  \\n8  N. Smuha, et al., above , at pp. 14 -15.;M. Veale and F.  Zuiderveen  Borgesius., Demystifying the draft EU AI a ct, 22(4) \\nComputer Law Review International, Ju ly 2021.  \\n9  See M. Ebers and others, above.  \\n10  See V. Galaz and others, Artificial intelligence, systemic risks, and sustainability , Vol 67, Technology in Society , 2021.   EPRS | European Parliamentary Research Service  \\n12  \\n11  For an overview, see T. Madiega and H. Mildebrath, Regulating facial recognition in the EU, 2021.  \\n12  The act applies to private organisations as well as to public authorities.  \\n13  The Annex refers  to AI systems used in areas of  critical infrastructures (e.g.  road traffic ), education and vocational \\ntraining , employment  worker management and access to self -employment , access to essential private and public \\nservices  and benefits  (e.g., creditworthiness evaluation ), law enforcement, border control, administration of justice \\nand democratic processes , biometric identification, categorisation and emotion recognit ion systems (outside the \\nprohibited categories) . \\n14  An AI system will not be considered as high -risk if one or more of the following criteria are fulfilled: (i) the AI system \\nis intended to perform a narrow procedural task; (ii) the AI system is intended t o improve the result of a previously \\ncompleted human activity; (iii) the AI system is intended to detect decision -making patterns or deviations from prior \\ndecision -making patterns and is not meant to replace or influence the previously completed human assessment \\nwithout proper human review; or (iv) the AI system is intended to perform a preparatory task to an assessment \\nrelevant for the purpose of the use cases listed in Annex III.  \\n15  Established by European  Commissi on decision in January  2024 the AI Office  enter s into force in February  2024.  \\n16   Furthermore , open -source models must comply with the AI act when they are integrated into prohibited AI \\npractices or into high -risk systems and when they are considered to present systemic risk . \\n17  FLOPs, or Floating -Point Operations Per Second, measur e a computer\\'s processing speed. The threshold should be \\nadjusted over time to reflect technological and industrial changes . Moreover, the Commission is entitled to  take \\nindividual decisions designating a GP AI mo del posing  systemic risk if it is found that such model has capabilities or \\nimpact equivalent to those captured by the FLOP threshold  on the basis of an overall assessment of criteria (e.g. \\nquality or size of the training data set, number of business and end users, degree of autonomy and scalability ). In the \\nUSA, President  Biden\\'s  AI executive order set 10^26 FLOPs as the threshold for AI models that need to be reported \\nto the government with details of their training, capabilities and security.  \\n18  Additional AI regulatory sandboxes at regional or local levels or jointly with other Member States\\' competent \\nauthorities may also be established . The European Data Protection Supervisor may also establish an AI regulatory \\nsandbox for the EU institutions, bodies and agencies.  \\n19  For instance, u p to €35 million  or 7 % of the total worldwide annual turnover of the preceding financial year \\n(whichever is higher) for infringements  on prohibited practices or non -compliance  related to requirements on data .  \\n20  Implementing acts must be adopted by the Commission to e stablish common specifications for req uirements for \\nhigh -risk systems, to a pprove codes of practice on g enerated or manipulated content and to specify common rules \\nfor implementation if such codes of practice are deemed not adequate. Delegated acts will need to be adopted to \\nidentify c onditions for AI system s to not be  considered  high -risk and to s pecify and update criteria of GPAI posing  \\nsystemic risk , inter alia. The AI Office will have to draw up the codes of practice for GPAI providers . \\n21  The Commission mandated the  European Standardisation Organisations  (CEN -CENELEC) to deliver a series of \\nEuropean standards to implement the AI act by January 2025.  \\n22  See P. Hacker , Comments on the f inal trilogue version of the AI a ct, 2024.  \\n23  See C. Kutterer, Regulati ng foundation models in the AI a ct: from \" high\" to \"systemic \" risk, 2024.  \\n24  See N. Helberger and others, The Amsterdam Paper: Recommendations for th e technical finalisation of the \\nregulation of GPAI in the AI a ct, 2024. See also , P. C havez, An AI c hallenge: Balancing o pen and c losed systems , 2023.  \\n25 See A. Engler,  The EU and U.S. diverge on AI regulation: A transatlantic comparison and steps to alignment , 2023.  \\n \\nDISCLAIMER AND COPYR IGHT  \\nThis document is prepared for, and addressed to, the Members and staff of the European Parliament as \\nbackground material to assist them in their parliamentary work. The content of the document is the sole \\nresponsibility of its author(s) and any opinions expressed herein should not be taken to represent an official \\nposition of the Parliament.  \\nReproduction and translation for non- commercial purposes are authorised, provided the source is \\nacknowledged and the European Parliament is given prior notice and sent a copy.  \\n© European Union, 202 4. \\neprs@ep.europa.eu  (contact)  \\nwww.eprs.ep.parl.union.eu  (intranet)  \\nwww.europarl.europa.eu/thinktank  (internet)  \\nhttp://epthinktank.eu  (blog)  \\nThird  edition. \\'EU Legislation in Progress \\' briefings are updated at key stages of the legislative procedure.  '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text_from_pdf('./EU-AI-Act.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
